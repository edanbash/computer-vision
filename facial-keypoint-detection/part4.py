# -*- coding: utf-8 -*-
"""part4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w7y55bjDZ218DO9w59nfcQZGlcckQTq1
"""

import cv2
import numpy as np
import torch
import os
import skimage.io as skio
import matplotlib.pyplot as plt
from torchvision import transforms, utils
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/My Drive/proj5

# Contains all the transform classes

import torchvision.transforms.functional as F
from ast import Index

class Rescale(object):
    """Rescale the image in a sample to a given size."""

    def __init__(self, output_size=(80,60)):
        self.output_size = output_size

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # Resize the image
        h, w = image.shape[:2]
        new_h, new_w = self.output_size
        img = cv2.resize(image, (new_h, new_w))

        sample['image'] = img

        return sample

class Normalize(object):
    """Normalize the image to float values in range -0.5 to 0.5"""

    def __init__(self):
        return

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # Normalize the image
        img = image.astype(np.float32) / 255 - 0.5
        sample['image'] = img

        return sample

class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        image, landmarks, heatmaps = sample['image'], sample['landmarks'], sample['heatmaps']

        # swap color axis because
        # numpy image: H x W x C
        # torch image: C x H x W

        if len(image.shape) == 3:
          image = image.transpose((2,0,1))
        sample['image'] = torch.from_numpy(image)
        sample['landmarks'] = torch.flatten(torch.from_numpy(landmarks))
        sample['heatmaps'] = torch.from_numpy(heatmaps)
        
        return sample

class RandomAffine(object):
    """Normalize the image to float values in range -0.5 to 0.5"""

    def __init__(self, translate=(0.2, 0.2), degrees=15):
        self.translate = translate
        self.degrees = degrees

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # Translate and rotate the image and keypoints
        h, w = image.shape[:2]
        angle = np.random.uniform(-self.degrees, self.degrees)
        dx = 0 #np.random.uniform(-w * self.translate[0], w * self.translate[0]) 
        dy = 0 #np.random.uniform(-h * self.translate[1], h * self.translate[1])

        img, ldmrks = warp_affine(image, landmarks, [dx,dy], angle)

        sample['image'] = img.astype(np.float32)
        sample['landmarks'] = ldmrks.astype(np.float32)
        sample['translate'] = [dx,dy]
        sample['angle'] = angle
        
        return sample

# Warps the image and landmark points based on rotation and translation

def warp_affine(image, landmarks, translate=[0,0], angle=0):
    # Rotate the image
    h, w = image.shape[:2]
    rot_matrix = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
    image = cv2.warpAffine(image, rot_matrix, (image.shape[1], image.shape[0]))

    # Translate the image
    tx, ty = translate
    translation_matrix = np.array([
      [1, 0, tx],
      [0, 1, ty]
    ], dtype=np.float32)
    image = cv2.warpAffine(image, translation_matrix, (image.shape[1], image.shape[0]))

    # Perform same transformation on landmarks
    new_landmarks = []
    landmarks = landmarks.reshape((len(landmarks),2))      
    for coord in landmarks:
        # Change coordinate from decimal to (x,y)
        coord = np.array(coord) * [w, h] 
        # Prepare the vector to be transformed 
        v = [coord[0],coord[1], 1]
        # Perform the actual rotation
        landmark = np.dot(rot_matrix, v)
        # Perform the image translation
        landmark = landmark[:2] + translate
        # Change landmark coords back to [0,1] values
        landmark = landmark / [w, h]
        # Append to resulting array
        new_landmarks.append(landmark)

    return image, np.array(new_landmarks)

import xml.etree.ElementTree as ET 
import numpy as np
import os 

# Reads in the image paths, landmarks, and bounding boxes
def read_large_data(root_dir, root):
  bboxes = [] # face bounding box used to crop the image
  landmarks = [] # the facial keypoints/landmarks for the whole training dataset
  img_filepaths = [] # the image names for the whole dataset

  for i, filename in enumerate(root[2]):
    filepath = os.path.join(root_dir, filename.attrib['file'])
    cropped_path = os.path.join('./large_face_database_cropped/', filepath[40:])
    if os.path.isfile(cropped_path):
      img_filepaths.append(filepath)
    
    box = filename[0].attrib
    # x, y for the top left corner of the box, w, h for box width and height
    bboxes.append([box['left'], box['top'], box['width'], box['height']]) 

    landmark = []
    for num in range(68):
      x_coordinate = int(filename[0][num].attrib['x'])
      y_coordinate = int(filename[0][num].attrib['y'])
      landmark.append([x_coordinate, y_coordinate])
    landmarks.append(landmark)
    if i%1000 == 0: print(f'Completed: {i}/6666')

  landmarks = np.array(landmarks).astype('float32')     
  bboxes = np.array(bboxes).astype('float32') 
  print(len(img_filepaths))
  return img_filepaths, landmarks, bboxes

import json

# Gets the bounding box of face based on landmark points
def get_bounding_box(image_size, landmarks, bbox):
  #h, left, top, w = bbox.astype(int)

  W, H = image_size

  x_coords = [landmark[0] for landmark in landmarks]
  y_coords = [landmark[1] for landmark in landmarks]

  # Get bounding box
  bound_left = int(min(x_coords))   #int(min(left, min(x_coords)))
  bound_top = int(min(y_coords))    #int(min(top, min(y_coords)))
  bound_right = int(max(x_coords))  #int(max(left+w, max(x_coords)))
  bound_bottom = int(max(y_coords)) #int(max(top+h, max(y_coords)))

  # Get new dimensions
  new_w, new_h = bound_right - bound_left, bound_bottom - bound_top

  # Add slight buffer to bounding box
  bound_left = max(0, int(bound_left - 0.1 * new_w))
  bound_right = min(W, int(bound_right + 0.1 * new_w))
  bound_top = max(0, int(bound_top - 0.1 * new_h))
  bound_bottom = min(H, int(bound_bottom + 0.1 * new_h))
  
  return [bound_left, bound_right, bound_top, bound_bottom]

# Crops the images and adjusts the landmark points
def crop(image, landmarks, bounds, crop_image=False):
  # Get bounding box points
  bound_left, bound_right, bound_top, bound_bottom = bounds
  # Get new dimensions
  new_w, new_h = bound_right - bound_left, bound_bottom - bound_top

  # Crop image
  if crop_image:
    return image[bound_top:bound_bottom, bound_left:bound_right]
  else:
    # Update landmark points
    new_landmarks = []
    for landmark in landmarks:
      new_landmark = np.array([landmark[0] - bound_left, landmark[1] - bound_top])
      new_landmark /= [new_w, new_h]
      new_landmarks.append(new_landmark)
    return np.array(new_landmarks)

def generate_heatmaps(landmarks, image_size=(224, 224)):
  heat_maps = []

  H, W = image_size

  # Create heatmaps for each landmark pt
  for pt in landmarks:
    heat_map = np.zeros(image_size)
    G = cv2.getGaussianKernel(10, sigma=10) 
    kernel = G @ G.T

    x = int(pt[0] * W)
    y = int(pt[1] * H)
    
    border = kernel.shape[0]//2

    left, right = max(0, x - border), min(W, x + border)
    top, bottom = max(0, y - border), min(H, y + border)

    #print(kernel[:bottom-top,:right-left])

    heat_map[top:bottom, left:right] += kernel[:bottom-top,:right-left]
    heat_maps.append(heat_map)
  
  return np.array(heat_maps).astype('float32')

# Custom Dataloader for Large Face Dataset

from operator import index
import pickle

original_root = './ibug_300W_large_face_landmark_dataset/'
cropped_root = './large_face_database_cropped/'

class LargeFaceDataset(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')
        root = tree.getroot()
        
        self.images, self.landmarks, self.bboxes = read_large_data(original_root, root)
        self.transform = transform
        self.cropped_images = pickle.load(open('cropped_metadata2.json', 'rb'))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        if type(idx) == torch.Tensor:
          idx = idx.item()

        filepath = cropped_root + self.images[idx][40:]
        image = cv2.imread(filepath, 0) 

        landmarks = np.array(self.cropped_images[filepath]['landmarks']).reshape((68,2))
        heatmaps = generate_heatmaps(landmarks) 
        
        bbox = np.array(self.cropped_images[filepath]['bounds'])

        sample = {'image': image, 'landmarks': landmarks, 'bbox': bbox, 'heatmaps': heatmaps}

        if self.transform:
            sample = self.transform(sample)

        return sample

# Create Large Face Dataset for display purposes
large_face_dataset = LargeFaceDataset(transform=None)

def visualize_heatmaps(heatmaps, single_map=False):
    map = None
    if single_map:
      map = heatmaps
    else:
      map = np.sum(heatmaps, axis=0)
    
    plt.imshow(map, cmap='hot', interpolation='nearest')
    plt.show()
    return map

def show_keypoints(image, landmarks):
    """Show image with nose key points"""
    h, w = image.shape[:2]
    for landmark in landmarks:
      #landmark *= [w, h]
      cv2.circle(image, landmark.astype(int), 1, (0,0,0), 2)

indices = np.random.choice(range(len(large_face_dataset)), 3)
for c, idx in enumerate(indices):
    sample = large_face_dataset[idx]
    image = sample['image']
    landmarks = sample['landmarks']
    heatmaps = sample['heatmaps']

    show_keypoints(image, landmarks)
    cv2.imwrite(f'./output/heatmap-image-{c}.jpg', image)

    result = visualize_heatmaps(heatmaps)
    cv2.imwrite(f'./output/heatmap-{c}.jpg', result)

# Create training and validation sets from large face dataset

large_face_dataset = LargeFaceDataset(transform=transforms.Compose([
    RandomAffine(degrees=10),
    Normalize(),
    ToTensor()
]))

train_size = int(0.8 * len(large_face_dataset))
test_size = len(large_face_dataset) - train_size
train_set, val_set = torch.utils.data.random_split(large_face_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(2))

train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)
val_loader = DataLoader(val_set, batch_size=16, shuffle=True, num_workers=2)

# Define UNet, loss function, and optimizer
pretrained_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, init_features=32, pretrained=True)

# Copied from https://github.com/mateuszbuda/brain-segmentation-pytorch

from collections import OrderedDict

import torch
import torch.nn as nn


class UNet(nn.Module):

    def __init__(self, in_channels=3, out_channels=1, init_features=32):
        super(UNet, self).__init__()

        features = init_features
        self.encoder1 = UNet._block(in_channels, features, name="enc1")
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder2 = UNet._block(features, features * 2, name="enc2")
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder3 = UNet._block(features * 2, features * 4, name="enc3")
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.encoder4 = UNet._block(features * 4, features * 8, name="enc4")
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.bottleneck = UNet._block(features * 8, features * 16, name="bottleneck")

        self.upconv4 = nn.ConvTranspose2d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name="dec4")
        self.upconv3 = nn.ConvTranspose2d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name="dec3")
        self.upconv2 = nn.ConvTranspose2d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name="dec2")
        self.upconv1 = nn.ConvTranspose2d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decoder1 = UNet._block(features * 2, features, name="dec1")

        self.conv = nn.Conv2d(
            in_channels=features, out_channels=out_channels, kernel_size=1
        )

    def forward(self, x):
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))
        enc4 = self.encoder4(self.pool3(enc3))

        bottleneck = self.bottleneck(self.pool4(enc4))

        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decoder4(dec4)
        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decoder3(dec3)
        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decoder2(dec2)
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decoder1(dec1)
        return self.conv(dec1)

    @staticmethod
    def _block(in_channels, features, name):
        return nn.Sequential(
            OrderedDict(
                [
                    (
                        name + "conv1",
                        nn.Conv2d(
                            in_channels=in_channels,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "norm1", nn.BatchNorm2d(num_features=features)),
                    (name + "relu1", nn.ReLU(inplace=True)),
                    (
                        name + "conv2",
                        nn.Conv2d(
                            in_channels=features,
                            out_channels=features,
                            kernel_size=3,
                            padding=1,
                            bias=False,
                        ),
                    ),
                    (name + "norm2", nn.BatchNorm2d(num_features=features)),
                    (name + "relu2", nn.ReLU(inplace=True)),
                ]
            )
        )

# Define UNET and load in pretrained weights
net = UNet()
net.load_state_dict(pretrained_model.state_dict())

# Definte loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.01, betas=(0.9, 0.999))

# Adjust the input and output layer sizes
net.encoder1.enc1conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
net.conv = nn.Conv2d(32, 68, kernel_size=(1, 1), stride=(1, 1))

# Move the model to the GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net.to(device)

# Saves the model to google drive
def save_checkpoint(model, optimizer, save_path, epoch):
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch
    }, save_path)

# Loads the model from google drive
def load_checkpoint(model, optimizer, load_path):
    checkpoint = torch.load(load_path, map_location=torch.device('cpu'))
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    
    return model, optimizer, epoch

# Training loop for large face dataset keypoint detection

training_loss = []
validation_loss = []
for epoch in range(10):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        # get the inputs; data is a list of [inputs, labels]
        images, labels = data['image'], data['heatmaps']
        images = images[:, None, :, :]
        images, labels = images.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Remove tensors from GPU
        del images
        del labels

        # print statistics
        if i%50 == 0: print(f'Batch {i}/{train_size//16+1}, Training Loss: {loss.item()}')
        running_loss += loss.item()
    
    avg_loss = running_loss / (i + 1)
    training_loss.append(avg_loss)

    running_vloss = 0.0
    for i, vdata in enumerate(val_loader):
        vinputs, vlabels = vdata['image'], vdata['heatmaps']
        vinputs = vinputs[:, None, :, :]
        vinputs, vlabels = vinputs.to(device), vlabels.to(device)

        voutputs = net(vinputs)
        vloss = criterion(voutputs, vlabels)
        running_vloss += vloss.item()

        # Remove tensors from GPU
        del vinputs
        del vlabels

        if i%50 == 0: print(f'Batch {i}/{test_size//16+1}, Validation Loss: {vloss.item()}')

    avg_vloss = running_vloss / (i + 1)
    validation_loss.append(avg_vloss)

    save_checkpoint(net, optimizer, 'unet_saved_model', epoch)

    print(f'Epoch: {epoch}, Training Loss: {avg_loss:.6f}, Val Loss: {avg_vloss:.6f}')

print('Finished Training')

# Loss Curve for large face dataset model

plt.title('Loss Curves')
plt.xlabel("Epoch")
plt.ylabel("Avg Loss")

plt.plot(range(10), training_loss, label="Training Loss")
plt.plot(range(10), validation_loss, label="Validation Loss")

plt.legend()
plt.show()

# Load Model from drive and put in eval mode
model, optimizer, epoch = load_checkpoint(net, optimizer, 'unet_saved_model')
model.eval()

import xml.etree.ElementTree as ET 

# Create the test_dataset
def create_test_set(test_dataset_file='./labels_ibug_300W_test_parsed.xml'):
  # Reading data from the xml file
  tree = ET.parse(test_dataset_file)
  root = tree.getroot()

  filepaths = []
  bboxes = []
  for i, filename in enumerate(root[2]):
    filepath = os.path.join(original_root, filename.attrib['file'])
    filepaths.append(filepath)

    box = filename[0].attrib
    bboxes.append([box['left'], box['top'], box['width'], box['height']])
  return filepaths, bboxes

test_filepaths, test_bboxes = create_test_set()
print(len(test_filepaths))

# Gets landmark points by taking softmax of model output and then doing weighted average
def get_landmark(heatmap):
  max_val = np.max(heatmap)
  soft_max = (np.exp(heatmap - max_val) / np.exp(heatmap - max_val).sum())

  h, w = heatmap.shape
  x_weights = np.sum(soft_max, axis=0) 
  y_weights = np.sum(soft_max, axis=1)

  x_prob = np.dot(x_weights, np.array(range(w))) / w
  y_prob = np.dot(y_weights, np.array(range(h))) / h

  return np.array([x_prob, y_prob])

def predict(image):
    # Normalize the image
    img = image.astype(np.float32) / 255 - 0.5
    # Reshape into [1,1,224,224] (B,C,H,W)
    img = img[None, None, :]
    # Convert into tensor
    img = torch.from_numpy(img)
    # Move image to GPU
    img = img.to(device)
    
    # Make keypoint predictions on image
    preds = model(img)
    preds = preds.detach().cpu().numpy()

    # Get all the landmarks from the heatmaps
    landmarks = []
    heatmaps = preds[0]
    for heatmap in heatmaps:
      landmark = get_landmark(heatmap)
      landmarks.append(landmark)
    return landmarks

# Rescale the prediction points to original image
def rescale_predictions(preds, bbox):
  left, top, bound_w, bound_h = np.array(bbox).astype(int)
  for pred in preds:
    pred[0] = pred[0] * bound_w + left
    pred[1] = pred[1] * bound_h + top
  return preds

# Display prediction on test set for large face dataset
def show_large_face_prediction(image, preds):
    """Show image with nose key points"""
    h, w = image.shape[:2]
    for pred in preds:
      #pred *= [w,h]
      cv2.circle(image, pred.astype(int), 1, (0,0,255), 7)

import os

test_dir = './test_set/'

# Predict on random test set image
indices = np.random.choice(range(len(test_filepaths)), 4)
for num, i in enumerate(indices):
  filepath = os.path.join(test_dir, test_filepaths[i].split('/')[-1])
  image = cv2.imread(filepath, 0)
  preds = predict(image)
    
  preds = rescale_predictions(preds, test_bboxes[i])
  original_img = cv2.imread(test_filepaths[i], 1)
  show_large_face_prediction(original_img, preds)
  cv2.imwrite(f'./output/unet_pred-{num}.jpg', original_img)

### KAGGLE PREDICTION ###

# Predict on random WHOLE test set
n = len(test_filepaths)
final_preds = []
for i in range(n):
  filepath = os.path.join(test_dir, test_filepaths[i].split('/')[-1])
  image = cv2.imread(filepath, 0)
  preds = predict(image)
    
  preds = rescale_predictions(preds, test_bboxes[i])
  final_preds.append(preds)

  if i%100 == 0: 
    print(f'Predicted {i}/{n}')
    original_img = cv2.imread(test_filepaths[i], 1)
    show_large_face_prediction(original_img, preds)
    cv2.imwrite(f'./output/kaggle-{i}.jpg', original_img)

import pandas as pd

kaggle_data = []
flattened_preds = np.array(final_preds).flatten()
print(len(flattened_preds))
for i, pred in enumerate(flattened_preds):
  kaggle_data.append([i, int(pred)])

df = pd.DataFrame(np.array(kaggle_data), columns = ['Id', 'Predicted'])
df.to_csv('unet_predictions.csv', index=False)

# Predict on custom images
custom_img_dir = './custom_images/'

custom_imgs = os.listdir(custom_img_dir)
for filename in custom_imgs:
  filepath = os.path.join(custom_img_dir, filename)
  image = cv2.imread(filepath, 0)
  img = cv2.resize(image, (224, 224))
  preds = predict(img)
  
  bbox = [0, 0, image.shape[0], image.shape[1]]
  preds = rescale_predictions(preds, bbox)
  original_img = cv2.imread(filepath, 1)
  show_large_face_prediction(original_img, preds)
  cv2.imwrite(f'./output/unet-pred-{filename}', original_img)