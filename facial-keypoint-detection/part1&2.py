# -*- coding: utf-8 -*-
"""part1&2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDdkWHDCWBliHOhz56TyQnxfvhgyGGlj
"""

import cv2
import numpy as np
import torch
import os
import skimage.io as skio
import matplotlib.pyplot as plt
from torchvision import transforms, utils
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/My Drive/proj5

# Read in all the images and point files for IMM Face Database

def read_images(root_dir):
    images, landmarks = [], []
    sorted_files = sorted(os.listdir(root_dir))
    
    for filename in sorted_files:
        if filename.endswith('.asf'):
          point_file = open(root_dir + filename)
          points = point_file.readlines()[16:74]
          landmark = []
          for point in points:
              x,y = point.split('\t')[2:4]
              landmark.append([float(x), float(y)])
          landmarks.append(landmark)
          
          image_file = root_dir + filename.split('.')[0] + '.jpg'
          images.append(cv2.imread(image_file))
        
    return images, np.array(landmarks).astype(np.float32)

# Contains all the transform classes

import torchvision.transforms.functional as F
from ast import Index

class Rescale(object):
    """Rescale the image in a sample to a given size."""

    def __init__(self, output_size=(80,60)):
        self.output_size = output_size

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # Resize the image
        h, w = image.shape[:2]
        new_h, new_w = self.output_size
        img = cv2.resize(image, (new_h, new_w))

        sample['image'] = img

        return sample

class Normalize(object):
    """Normalize the image to float values in range -0.5 to 0.5"""

    def __init__(self):
        return

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # Normalize the image
        img = image.astype(np.float32) / 255 - 0.5
        sample['image'] = img

        return sample

class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # swap color axis because
        # numpy image: H x W x C
        # torch image: C x H x W

        if len(image.shape) == 3:
          image = image.transpose((2,0,1))
        sample['image'] = torch.from_numpy(image)
        sample['landmarks'] = torch.flatten(torch.from_numpy(landmarks))
        
        return sample

class ColorJitter(object):
    def __init__(self, brightness=0.5, saturation=0.5):
        self.brightness = brightness
        self.saturation = saturation

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        if len(image.shape) == 3:
          image = image.transpose((2,0,1))
        
        image = torch.from_numpy(image)

        # Adjust brightness and saturation of image
        brightness = np.random.uniform(low=max(0, 1 - self.brightness), high=1 + self.brightness)
        img = transforms.functional.adjust_brightness(image, brightness)
        saturation = np.random.uniform(low=max(0, 1 - self.saturation), high=1 + self.saturation)
        img = transforms.functional.adjust_saturation(img, saturation)

        if len(image.shape) == 3:
          img = img.numpy().transpose((1,2,0))
        
        return {'image': img, 'landmarks': landmarks}

class RandomAffine(object):
    """Normalize the image to float values in range -0.5 to 0.5"""

    def __init__(self, translate=(0.2, 0.2), degrees=15):
        self.translate = translate
        self.degrees = degrees

    def __call__(self, sample):
        image, landmarks = sample['image'], sample['landmarks']

        # Translate and rotate the image and keypoints
        h, w = image.shape[:2]
        angle = np.random.uniform(-self.degrees, self.degrees)
        dx = 0#np.random.uniform(-w * self.translate[0], w * self.translate[0]) 
        dy = 0#np.random.uniform(-h * self.translate[1], h * self.translate[1])

        img, ldmrks = warp_affine(image, landmarks, [dx,dy], angle)

        sample['image'] = img.astype(np.float32)
        sample['landmarks'] = ldmrks.astype(np.float32)
        sample['translate'] = [dx,dy]
        sample['angle'] = angle
        
        return sample

# Custom Dataloader for IMMFaceDataset

class IMMFaceDataset(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, root_dir='./imm_face_db/', transform=None, nose_only=True):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.images, self.landmarks = read_images(root_dir)
        self.root_dir = root_dir
        self.transform = transform
        self.nose_only = nose_only

    def __len__(self):
        return len(self.landmarks)

    def __getitem__(self, idx):
        if type(idx) == torch.Tensor:
          idx = idx.item()

        image = self.images[idx]
        landmarks = self.landmarks[idx]

        sample = {'image': image, 'landmarks': landmarks}
        
        if self.nose_only:
          nose_keypoint = np.array(landmarks).astype('float32')[-6] 
          sample = {'image': image, 'landmarks': nose_keypoint} 

        if self.transform:
            sample = self.transform(sample)

        return sample

# Create dataset for display purposes
face_dataset = IMMFaceDataset(transform=Rescale())

# Create sample nose ground truth images
def show_nose(image, nose):
    """Show image with nose key points"""
    h, w = image.shape[:2]
    nose *= [w, h]
    cv2.circle(image, nose.astype(int), 1, (0,255,0), 2)

indices = np.random.choice(range(len(face_dataset)), 4)
# Sample a few images and display them along with the nose keypoints.
for c, idx in enumerate(indices):
    sample = face_dataset[idx]
    show_nose(sample['image'], sample['landmarks'])
    cv2.imwrite(f'./output/ground-truth-{c}.jpg', sample['image'])

# Create the training and validation sets for nose detection
face_dataset = IMMFaceDataset(transform=transforms.Compose([Rescale(), Normalize(), ToTensor()]))

train_size = int(0.8 * len(face_dataset))
test_size = len(face_dataset) - train_size
train_set, val_set = torch.utils.data.random_split(face_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(1))

train_set = DataLoader(train_set)
val_set = DataLoader(val_set)

# PART 1: Custom CNN for Nose Detection

import torch.nn as nn
import torch.nn.functional as F

class NoseDetectorNet(nn.Module):
    def __init__(self):
        super(NoseDetectorNet, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        self.conv1 = nn.Conv2d(3, 6, 3)
        self.conv2 = nn.Conv2d(6, 8, 3)
        self.conv3 = nn.Conv2d(8, 12, 3)
        self.conv4 = nn.Conv2d(12, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(48, 120)  # 5*8 from image dimension
        self.fc2 = nn.Linear(120, 2)   # output is x and y position

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = F.max_pool2d(F.relu(self.conv3(x)), 2)
        x = F.max_pool2d(F.relu(self.conv4(x)), 2)

        x = torch.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

# Define CNN, loss function, and optimizer
net = NoseDetectorNet()
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))

# Training Loop for Nose Detection

training_loss = []
validation_loss = []
for epoch in range(20):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(train_set, 0):
        # get the inputs; data is a list of [inputs, labels]
        image, label = data['image'], data['landmarks']

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        output = net(image)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
    
    avg_loss = running_loss / (i + 1)
    training_loss.append(avg_loss)

    running_vloss = 0.0
    for i, vdata in enumerate(val_set):
        vinputs, vlabels = vdata['image'], vdata['landmarks']
        voutputs = net(vinputs)
        vloss = criterion(voutputs, vlabels)
        running_vloss += vloss.item()

    avg_vloss = running_vloss / (i + 1)
    validation_loss.append(avg_vloss)

    print(f'Epoch: {epoch}, Training Loss: {avg_loss:.6f}, Val Loss: {avg_vloss:.6f}')

print('Finished Training')

# Loss Curve for Nose Detection

plt.title('Loss Curves')
plt.xlabel("Epoch")
plt.ylabel("Avg Loss")

plt.plot(range(19), training_loss[1:], label="Training Loss")
plt.plot(range(20), validation_loss, label="Validation Loss")

plt.legend()
plt.show()

# Display some of the nose predictions 

def show_prediction(image, pred, nose):
    """Show image with nose key points"""
    h, w = image.shape[:2]
    nose *= [w, h]
    pred *= [w, h]
    cv2.circle(image, nose.astype(int), 1, (0,255,0), 2)
    cv2.circle(image, pred.astype(int), 1, (0,0,255), 2)

original_dataset = IMMFaceDataset(transform=Rescale())
indices = np.random.choice(range(len(original_dataset)), 4)

preds = []
for c, idx in enumerate(indices):
    sample = face_dataset[idx]
    image = sample['image']
    nose = sample['landmarks']
    pred = net(image)
    print(nose, pred)
    preds.append(pred.detach().numpy())

for c, idx in enumerate(indices):
    sample = original_dataset[idx]
    image = sample['image']
    nose = sample['landmarks']
    show_prediction(image, preds[c], nose)
    cv2.imwrite(f'./output/node-pred-{c}.jpg', sample['image'])

# Warps the image and landmark points based on rotation and translation

def warp_affine(image, landmarks, translate=[0,0], angle=0):
    # Rotate the image
    h, w = image.shape[:2]
    rot_matrix = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
    image = cv2.warpAffine(image, rot_matrix, (image.shape[1], image.shape[0]))

    # Translate the image
    tx, ty = translate
    translation_matrix = np.array([
      [1, 0, tx],
      [0, 1, ty]
    ], dtype=np.float32)
    image = cv2.warpAffine(image, translation_matrix, (image.shape[1], image.shape[0]))

    # Perform same transformation on landmarks
    new_landmarks = []
    landmarks = landmarks.reshape((len(landmarks),2))      
    for coord in landmarks:
        # Change coordinate from decimal to (x,y)
        coord = np.array(coord) * [w, h] 
        # Prepare the vector to be transformed 
        v = [coord[0],coord[1], 1]
        # Perform the actual rotation
        landmark = np.dot(rot_matrix, v)
        # Perform the image translation
        landmark = landmark[:2] + translate
        # Change landmark coords back to [0,1] values
        landmark = landmark / [w, h]
        # Append to resulting array
        new_landmarks.append(landmark)

    return image, np.array(new_landmarks)

# Define the transformed IMM Face Database for display purposes

face_dataset = IMMFaceDataset(transform=transforms.Compose([
      Rescale((240, 180)), 
      ColorJitter(),
      RandomAffine(),
]), nose_only=False)

# Create sample face keypoint ground truth images

def show_keypoints(image, landmarks):
    """Show image with nose key points"""
    h, w = image.shape[:2]
    for landmark in landmarks:
      landmark *= [w, h]
      cv2.circle(image, landmark.astype(int), 1, (0,255,0), 2)

indices = np.random.choice(range(len(face_dataset)), 4)
# Sample a few images and display them along with the nose keypoints.
for c, idx in enumerate(indices):
    sample = face_dataset[idx]
    show_keypoints(sample['image'], sample['landmarks'])
    cv2.imwrite(f'./output/augmented-data-{c}.jpg', sample['image'])

# # PART 2: Custom CNN for Face Keypoint Detection

import torch.nn as nn
import torch.nn.functional as F

class FaceKeypointDetectorNet(nn.Module):

    def __init__(self):
        super(FaceKeypointDetectorNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.conv2 = nn.Conv2d(6, 12, 5)
        self.conv3 = nn.Conv2d(12, 16, 5)
        self.conv4 = nn.Conv2d(16, 24, 5)
        self.conv5 = nn.Conv2d(24, 28, 5)
        self.fc1 = nn.Linear(49000, 400)    
        self.fc2 = nn.Linear(400, 58 * 2)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = F.relu(self.conv3(x))
        x = F.max_pool2d(F.relu(self.conv4(x)), 2)
        x = F.relu(self.conv5(x))

        x = torch.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

# Create the training and validation sets for face keypoint detection

face_dataset = IMMFaceDataset(transform=transforms.Compose([
      Rescale((240, 180)), 
      ColorJitter(),
      RandomAffine(),
      Normalize(),
      ToTensor(),
]), nose_only=False)


train_size = int(0.8 * len(face_dataset))
test_size = len(face_dataset) - train_size
train_set, val_set = torch.utils.data.random_split(face_dataset, [train_size, test_size], generator=torch.Generator().manual_seed(2))

train_set = DataLoader(train_set)
val_set = DataLoader(val_set)

# Define CNN, loss function, and optimizer

net = FaceKeypointDetectorNet()
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))

# Training Loop for facial keypoint detection

training_loss = []
validation_loss = []
for epoch in range(20):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(train_set, 0):
        # get the inputs; data is a list of [inputs, labels]
        image, labels = data['image'], data['landmarks']

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        output = net(image)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
    
    avg_loss = running_loss / (i + 1)
    training_loss.append(avg_loss)

    running_vloss = 0.0
    for i, vdata in enumerate(val_set):
        vinputs, vlabels = vdata['image'], vdata['landmarks']
        voutputs = net(vinputs)
        vloss = criterion(voutputs, vlabels)
        running_vloss += vloss.item()

    avg_vloss = running_vloss / (i + 1)
    validation_loss.append(avg_vloss)

    print(f'Epoch: {epoch}, Training Loss: {avg_loss:.6f}, Val Loss: {avg_vloss:.6f}')

print('Finished Training')

# Loss Curve for Facial Keypoint Detection

plt.title('Loss Curves')
plt.xlabel("Epoch")
plt.ylabel("Avg Loss")

plt.plot(range(19), training_loss[1:], label="Training Loss")
plt.plot(range(20), validation_loss, label="Validation Loss")

plt.legend()
plt.show()

# Display some of the face keypoint predictions 

def show_face_prediction(image, landmarks, preds):
    """Show image with nose key points"""
    h, w = image.shape[:2]
    for i in range(len(landmarks)):
      landmarks[i] *= [w, h]
      preds[i] *= [w, h]
      
      cv2.circle(image, landmarks[i].astype(int), 1, (0,255,0), 2)
      cv2.circle(image, preds[i].astype(int), 1, (0,0,255), 2)
    
original_dataset = IMMFaceDataset(transform=Rescale((240, 180)), nose_only=False)
indices = np.random.choice(range(len(original_dataset)), 4)

for c, idx in enumerate(indices):
    print(f'Person {c}')
    # Get predictions from nerual net
    sample1 = face_dataset[idx]
    image, landmarks = sample1['image'], sample1['landmarks']
    preds = net(image)
    preds = preds.detach().numpy().reshape((58,2))

    landmarks = np.array(landmarks.reshape((58,2)))
    #for i in range(len(landmarks)):
      #print(landmarks[i], preds[i])

    # Get original landmark data
    sample2 = original_dataset[idx]
    image = sample2['image']
    image, landmarks = warp_affine(image, sample2['landmarks'], 0, sample1['angle'])
    
    show_face_prediction(image, landmarks, preds)
    cv2.imwrite(f'./output/keypoint-pred-{c}.jpg', image)

# Visualize the learned filters of the CNN

def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): 
    n,c,w,h = tensor.shape

    if allkernels: tensor = tensor.view(n*c, -1, w, h)
    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)

    rows = np.min((tensor.shape[0] // nrow + 1, 64))    
    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)
    plt.figure( figsize=(nrow,rows) )
    plt.imshow(grid.numpy().transpose((1, 2, 0)))

layers = [net.conv1, net.conv2, net.conv3, net.conv4, net.conv5]
for layer in layers:
    filter = layer.weight.data.clone()
    visTensor(filter, ch=0, allkernels=False)

    plt.axis('off')
    plt.ioff()
    plt.show()